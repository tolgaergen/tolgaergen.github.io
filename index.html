
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="Academic Website">
    <meta name="author" content="Tolga Ergen">
    <meta name="keywords" content="Stanford, Optimization, Deep Learning, Neural Networks, Research">
    <title>Tolga Ergen</title>
    <link href="css/bootstrap.min2.css" rel="stylesheet">
    <link href="css/non-responsive.css" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
  
  
  </head>

  <body>
    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Tolga Ergen</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="research.html">Research</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
    	  <div class="page-header">
      		<h2>Tolga Ergen </h2>
      	</div>
      	
      	<div class="col-md-5 col-sm-5 col-xs-5">
      		<img src="images/tolga.jpg" class="col-md-12 col-sm-10 col-xs-10" title="Tolga Ergen"/>
      	</div>
      
      	<div class="col-md-7 col-sm-5 col-xs-6">
 I am currently a Research Scientist at <a href="https://www.lgresearch.ai/">LG AI Research</a>. Prior to that, I also had the opportunity to work with <a href="https://www.neyshabur.net">Behnam Neyshabur</a> and <a href="https://scholar.google.com/citations?user=murJPNoAAAAJ&hl=en">Harsh Mehta</a> at <a href="https://research.google/">Google Research</a>, and <a href="https://yubai.org">Yu Bai</a>  at <a href="https://einstein.ai/">Salesforce Research</a> as a research intern.
            <br>
          <br/>
               My main research interests are in <strong> optimization</strong> and <strong>deep learning</strong>. Currently, I focus on improving optimization, efficiency, and understanding of Large Language Models (LLMs).
          <br>
    			<br/>
          I received my Ph.D. in <a href="https://ee.stanford.edu/">Electrical Engineering</a> from <a href="https://www.stanford.edu/">Stanford University</a> in 2023. Prior to joining Stanford, I obtained B.S. and M.S. degrees in <a href="http://www.ee.bilkent.edu.tr/">Electrical and Electronics Engineering</a>
              from <a href="https://w3.bilkent.edu.tr/bilkent/">Bilkent University</a> in 2016 and 2018, respectively, where I worked with
              Prof. <a href="http://kilyos.ee.bilkent.edu.tr/~kozat/">Suleyman Serdar Kozat</a>.
          <br>
    			<br/>
  			
      	</div>
    </div>
        

             <p style="text-align:center;font-size:18px" >
             <a class="link link_email" href="mailto:ergen@stanford.edu"><i class="fa fa-envelope big-icon" style="font-size:28px"></i><font size="5">Email</font></a> &nbsp
             <a class="link link_cv" href="data/CV_Tolga_Ergen.pdf"><i class="fa fa-file big-icon" style="font-size:28px"></i><font size="5">CV</font></a> &nbsp
             <a class="link link_scholar" href="https://scholar.google.com.tr/citations?user=T1pWaCsAAAAJ&hl=en"><i class="fa fa-graduation-cap big-icon" style="font-size:28px"></i><font size="5">Scholar</font></a> &nbsp
             <a class="link link_twitter" href="https://twitter.com/tolgaergen_"><i class="fa fa-twitter big-icon" style="font-size:28px"></i><font size="5">Twitter</font></a> &nbsp
             <a class="link link_git" href="https://github.com/tolgaergen" ><i class="fa fa-github big-icon" style="font-size:28px"></i><font size="5">Github</font></a> 
           </p>
             
    <div class="container">
		<h2>News</h2>
    	<ul>
         <li>(Sep 2023) Two papers accepted at <a href="https://neurips.cc/"> <strong> NeurIPS 2023</strong> </a> </a>: We analyze <a href="https://openreview.net/pdf?id=Z1Aj59LoZD">pathwise regularized deep neural network training problems</a> through convex optimization and relate similar analyses to the <a href="https://openreview.net/pdf?id=YWsPN0EMZr">NTK</a> of deep neural networks.</li>
         <li>(Jul 2023) I joined <a href="https://www.lgresearch.ai/">LG AI Research</a> as a Research Scientist.</li>
         <li>(Jun 2023) I successfully defended my Ph.D. dissertation and graduated from Stanford University.</li>
         <li>(Jan 2023) Two papers accepted at <a href="https://iclr.cc/"> <strong> ICLR 2023</strong> </a> </a>: We study the globally optimal training of <a href="https://openreview.net/forum?id=_9k5kTgyHT"> threshold/binary networks</a> and <a href="https://openreview.net/forum?id=6zrOr_Rdhjs">duality gap</a> of deep neural networks.</li>
        <li>(Nov 2022) A new preprint out! We develop a convex optimization perspective for <a href="https://arxiv.org/pdf/2211.11052.pdf"> transformer networks</a>.</li>
        <li>(June 2022) I joined <a href="https://research.google/">Google Research</a> as a research intern.</li>
        <li>(May 2022) Our paper accepted at <a href="https://icml.cc/"> <strong> ICML 2022</strong> </a> </a>: We analyze <a href="https://arxiv.org/pdf/2205.08078.pdf"> attention</a> in vision transformers via convex optimization.</li>
        <li>(Feb 2022) I've been awarded <a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/"> <strong> 2022 Adobe Research Fellowship</strong> </a>.
        <li>(Jan 2022) Two papers accepted at <a href="https://iclr.cc/"> <strong> ICLR 2022</strong> </a> </a>: We analyze <a href="https://openreview.net/pdf?id=6XGgutacQ0B"> Batchnorm</a>  and <a href="https://openreview.net/pdf?id=e2Lle5cij9D">GANs</a> via convex optimization.</li>
        <li>(Nov 2021) I will give a talk at <a href="https://deepmath-conference.com/"> <strong> DeepMath 2021</strong> </a></a> as the  <strong>best poster award winner</strong>.
        <li>(Oct 2021) I was selected to receive a <a href="https://neurips.cc/"> <strong>NeurIPS 2021 Outstanding Reviewer Award </strong></a>  given to <strong>the top 8% of reviewers</strong>.
        <li>(Sep 2021) New arXiv preprint: We analyze <a href="https://arxiv.org/pdf/2110.09548.pdf">path regularized ReLU networks </a> via convex duality.</li>
        <li>(Sep 2021) A new preprint out! We exactly characterize <a href="https://arxiv.org/pdf/2110.06482.pdf"> duality gap for parallel deep networks</a>.</li>
       	<li>(Aug 2021) Our paper on <a href="https://jmlr.org/papers/v22/20-1447.html">convex analysis of ReLU networks </a> has been accepted to JMLR.</li>
   	    <li>(June 2021) I received <strong>the outstanding paper award </strong> at <a href="https://www.2021.ieeeicassp.org/2021.ieeeicassp.org/default.html"> <strong> ICASSP 2021</strong> </a> </a>.
        <li>(June 2021) I joined <a href="https://einstein.ai/">Salesforce Research</a> as a research intern.</li>
        <li>(July 2021) A new preprint out! We analyze the training of <a href="https://arxiv.org/pdf/2107.05680.pdf">Wasserstein GANs</a> through the lens of convex duality.</li>
        <li>(May 2021) Two papers accepted at <a href="https://icml.cc/"><strong> ICML 2021</strong> </a>: Topics include <a href="http://proceedings.mlr.press/v139/ergen21a">globally optimal training </a>  and <a href="http://proceedings.mlr.press/v139/ergen21b">convex analysis</a> of deep neural networks. </li>
        <li>(Jan 2021) Two papers accepted at <a href="https://iclr.cc/"> <strong> ICLR 2021</strong> </a> </a>: We introduce exact convex programs for <a href="https://openreview.net/pdf?id=0N8jUH4JMv6"> convolutional</a>  and <a href="https://openreview.net/pdf?id=fGF8qAqpXXG">vector-output</a> ReLU networks.</li>
  	     <li>(Dec 2020) I received <strong>the best poster award </strong> at <a href="https://deepmath-conference.com/"> <strong> DeepMath 2020</strong>  </a>.

   	</ul>
 	</div>
	
	<div class="container">
		<h2>Education</h2>

   <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td style="padding:20px;width:15%;vertical-align:middle"><img src="images/stanford.png" style="width:100px;height:100px;"></td>
            <td width="70%" valign="center">
             Ph. D., <a href="https://ee.stanford.edu/", target="_blank">Electrical Engineering</a>, <a href="https://www.stanford.edu", target="_blank">Stanford University</a>, 2018-2023<br/>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src="images/bilkent.png" style="width:100px;height:100px;" >
            </td>
             <td width="75%" valign="center">
            	M. S., <a href="https://ee.bilkent.edu.tr/en/">Electrical and Electronics Engineering</a>, <a href="https://w3.bilkent.edu.tr/bilkent/", target="_blank">Bilkent University</a>, 2018<br/> 
            	B. S., <a href="https://ee.bilkent.edu.tr/en/">Electrical and Electronics Engineering</a>, <a href="https://w3.bilkent.edu.tr/bilkent/", target="_blank">Bilkent University</a>, 2016<br/> 
            </td>
          </tr>
        </tbody></table>
	</div>
	<div class="container">
		<h2>Work Experience</h2>

   <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:15%;vertical-align:middle"><img src="images/LG_logo.png" style="width:100px;height:100px;"></td>
            <td width="70%" valign="center">
             Research Scientist, <a href="https://www.lgresearch.ai/", target="_blank">LG AI Research</a>,  Jul 2023 - Present<br/>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:15%;vertical-align:middle"><img src="images/google_logo.png" style="width:100px;height:100px;"></td>
            <td width="70%" valign="center">
             Research Intern, <a href="https://research.google/", target="_blank">Google Research</a>,  Jun 2022 - Sept 2022<br/>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src="images/salesforce_logo.png" style="width:100px;height:100px;" >
            </td>
             <td width="75%" valign="center">
            	Research Intern, <a href="https://einstein.ai/", target="_blank">Salesforce Research</a>, Jun 2021 - Sept 2021<br/> 
            </td>
          </tr>
        </tbody></table>
	</div>
  <div class="container">
    <h2>Teaching</h2>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td style="padding:20px;width:15%;vertical-align:middle"><img src="images/stanford.png" style="width:100px;height:100px;"></td>
            <td width="70%" valign="center">
              (Fall 2019, 2020, 2021) <a href="http://web.stanford.edu/class/ee269/", target="_blank">EE269</a>, Signal Processing for Machine Learning, TA
              <br>
              (Winter 2020, 2021) <a href="http://web.stanford.edu/class/ee270/", target="_blank">EE270</a>, Large Scale Matrix Computation, Optimization and Learning, TA
              <br>
              (Spring 2020, 2021) <a href="http://web.stanford.edu/class/ee364b/", target="_blank">EE364B</a>,  Convex Optimization II, TA
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src="images/bilkent.png" style="width:100px;height:100px;" >
            </td>
             <td width="75%" valign="center">
              (Fall 2016, 2017, Spring 2018) <a href="https://stars.bilkent.edu.tr/syllabus/view/EEE/424/EE_BS/", target="_blank">EEE424</a>,  Digital Signal Processing, TA
              <br>
              (Spring 2018) <a href="https://stars.bilkent.edu.tr/syllabus/view/EEE/102/20151?section=1", target="_blank">EEE102</a>, Introduction to Digital Circuit Design, TA
            </td>
          </tr>
        </tbody></table>
  </div>

  <div class="container">
    <h2>Academic Service</h2>
             <ul> Reviewer for NeurIPS, ICML, ICLR, IEEE TNNLS, IEEE SPL</ul>
  </div>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>


