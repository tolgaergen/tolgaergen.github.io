<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="Academic Website">
    <meta name="author" content="Tolga Ergen">
    <meta name="keywords" content="Stanford, Optimization, Deep Learning, Neural Networks, Research">
    <title>Tolga Ergen</title>
    <link href="css/bootstrap.min2.css" rel="stylesheet">
    <link href="css/non-responsive.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
  </head>
  <body>
    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Tolga Ergen</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li class="active"><a href="#">Research</a></li>
	    
			<!--/<li><a href="software.html">software</a></li>-->
	   
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>
    
    <div class="container">
    	<hr>
		<h3>Research interests</h3>
		<ul>
		  <li> <b>Deep neural networks</b>: Can we better understand neural networks through the lens of convex optimization theory?</li>
		  <li> <b>Optimization</b>: How can we develop optimization and training strategies that have low computational complexity and strong performance guarantees to enable efficient training of deep learning models? </li>
		</ul>
    </div>

	<div class="container">
	<hr>
      	<h3>Publications</h3>
	      	<table style="width:100%;border:10px;border-spacing:00px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
            <td width="30%" >
           
            </td>
          </tr>
        <table style="width:100%;border:10px;border-spacing:30px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
<tr>
          <td width="25%">
            <div class="one">
                <img src="images/BM.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
      <p><a href="https://openreview.net/forum?id=ikmuHqugN7">
        <papertitle><strong>Scaling Convex Neural Networks with Burer-Monteiro Factorization
</strong></papertitle></a><br>
 Arda Sahiner, <strong>Tolga Ergen</strong>, Batu Ozturkler, John Pauly, Morteza Mardani, Mert Pilanci<br/> 
                          <em>ICLR 2024 </em>&nbsp; <br> </p>
        <a class="link link_pdf" rel="external" href="https://openreview.net/forum?id=ikmuHqugN7">Proceeding</a>                        
           </p>
      
              </p>
     <p>
              <a class="keyword"> Burer Monteiro Factorization </a>
              <a class="keyword"> convex optimizations </a>
              <a class="keyword"> neural networks </a>
                </p>
              </td>
            </tr>

<tr>
          <td width="25%">
            <div class="one">
                <img src="images/weight_norm.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
      <p><a href="https://arxiv.org/abs/2312.12657">
        <papertitle><strong>The Convex Landscape of Neural Networks: Characterizing Global Optima and Stationary Points via Lasso Models
</strong></papertitle></a><br>
 <strong>Tolga Ergen</strong>, Mert Pilanci<br/> </p>
    <a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/2312.12657">arXiv</a>
           </p>
      
              </p>
     <p>
              <a class="keyword"> convex optimization </a>
              <a class="keyword">  sparse models </a>
              <a class="keyword"> deep neural networks </a>
                </p>
              </td>
            </tr>

  <tr>
          <td width="25%">
            <div class="one">
                <img src="images/path.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
      <p><a href="https://arxiv.org/abs/2110.09548">
        <papertitle><strong>Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/> 
                          <em>NeurIPS 2023 </em>&nbsp; <br> </p>
    <a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2110.09548">arXiv</a>
        <a class="link link_pdf" rel="external" href="https://openreview.net/forum?id=Z1Aj59LoZD">Proceeding</a>                        
               </p>
     <p>
              <a class="keyword"> deep neural networks </a>
              <a class="keyword"> convex optimization </a>
              <a class="keyword"> path norm </a>

     </p>
              </td>
            </tr>

    <tr>
          <td width="25%">
            <div class="one">
                <img src="images/fixing_the_ntk.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
      <p><a href="https://arxiv.org/abs/2309.15096">
        <papertitle><strong>Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs</strong></papertitle></a><br>
  Rajat Dwaraknath, <strong>Tolga Ergen</strong>, Mert Pilanci<br/> 
                          <em>NeurIPS 2023 </em>&nbsp; <br> </p>
    <a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2309.15096">arXiv</a>   
        <a class="link link_pdf" rel="external" href="https://openreview.net/forum?id=YWsPN0EMZr">Proceeding</a>                     
               </p>
     <p>
              <a class="keyword"> deep neural networks </a>
              <a class="keyword"> convex optimization </a>
              <a class="keyword"> neural tangent kernel (NTK) </a>

     </p>
              </td>
            </tr>

   	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/threshold.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://openreview.net/forum?id=_9k5kTgyHT">
        <papertitle><strong>Globally Optimal Training of Neural Networks with Threshold Activation Functions</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Ibrahim Gulluk, Jonathan Lacotte, Mert Pilanci<br/>
                          <em>ICLR 2023 </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2303.03382">arXiv</a>
 		<a class="link link_pdf" rel="external" href="https://openreview.net/forum?id=_9k5kTgyHT">Proceeding</a>          

	           </p>
     <p>
              <a class="keyword"> threshold/binary activations </a>
              <a class="keyword"> deep neural networks </a>

     </p>
              </td>
            </tr>
    
 	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/dual_gap.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://arxiv.org/abs/2110.06482">
        <papertitle><strong>Parallel Deep Neural Networks Have Zero Duality Gap</strong></papertitle></a><br>
Yifei Wang, <strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                          <em>ICLR 2023 </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2110.06482">arXiv</a>
 		<a class="link link_pdf" rel="external" href="https://openreview.net/forum?id=6zrOr_Rdhjs">Proceeding</a>          

	           </p>
     <p>
              <a class="keyword"> duality gap </a>
              <a class="keyword"> convex optimization </a>
              <a class="keyword"> deep neural networks </a>

     </p>
              </td>
            </tr>
      
<tr>
          <td width="25%">
            <div class="one">
                <img src="images/attention3.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://arxiv.org/abs/2205.08078">
        <papertitle><strong>Convexifying Transformers: Improving optimization and
understanding of transformer networks</strong></papertitle></a><br>
 <strong>Tolga Ergen</strong>, Behnam Neyshabur, Harsh Mehta<br/> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/2211.11052">arXiv</a>
           </p>
      
              </p>
     <p>
              <a class="keyword"> self-attention </a>
              <a class="keyword">  transformer </a>
              <a class="keyword"> convex optimization </a>
                </p>
              </td>
            </tr>

<tr>
          <td width="25%">
            <div class="one">
                <img src="images/constrained_nuclear.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://arxiv.org/abs/2205.08078">
        <papertitle><strong>Unraveling Attention via Convex Duality:
Analysis and Interpretations of Vision Transformers</strong></papertitle></a><br>
Arda Sahiner, <strong>Tolga Ergen</strong>, Batu Ozturkler, John Pauly, Morteza Mardani, Mert Pilanci<br/> 
                      <em>ICML 2022 </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/2205.08078">arXiv</a>
		<a class="link link_pdf" rel="external" href="https://proceedings.mlr.press/v162/sahiner22a.html">Proceeding</a>          
           </p>
      
              </p>
     <p>
              <a class="keyword"> attention </a>
              <a class="keyword"> vision transformer </a>
              <a class="keyword"> convex duality </a>
                </p>
              </td>
            </tr>
   

         
      
           <html>




      <tr>
          <td width="30%" >
            <div class="one">
                <img src="images/batchnorm.png" style="width:200px;height:150px;">
            </div>                
              </td>
  
              <td valign="middle" width="70%">
		  <p><a href="https://arxiv.org/abs/2103.01499">
        <papertitle><strong>Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization</strong></papertitle></a><br>
<strong>Tolga Ergen*</strong>, Arda Sahiner*, Batu Ozturkler, John Pauly, Morteza Mardani, Mert Pilanci<br/> 
                      <em>ICLR 2022 </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2103.01499">arXiv</a>
		<a class="link link_pdf" rel="external" href="https://openreview.net/pdf?id=6XGgutacQ0B">Proceeding</a>              </p>
              </p>
                  <p></p>
              <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex analysis </a>
              </p>
              </td>
            </tr>
   
   
   
   	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_gan.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://arxiv.org/abs/2107.05680">
        <papertitle><strong>Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions</strong></papertitle></a><br>
Arda Sahiner*, <strong>Tolga Ergen*</strong>, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, Mert Pilanci<br/> 
                      <em>ICLR 2022 </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/2107.05680">arXiv</a>
		<a class="link link_pdf" rel="external" href="https://openreview.net/pdf?id=e2Lle5cij9D">Proceeding</a>              </p>
      
              </p>
     <p>
              <a class="keyword"> generative adversarial networks </a>
              <a class="keyword"> convex-concave games </a>
              <a class="keyword"> convex duality </a>
                </p>
              </td>
            </tr>
   
   
          
      	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_dualgeo2.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://jmlr.org/papers/v22/20-1447.html">
        <papertitle><strong>Convex Geometry and Duality of Over-parameterized Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>JMLR </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/2002.11219">arXiv</a>             
		<a class="link link_pdf" rel="external" href="https://jmlr.org/papers/v22/20-1447.html">Proceeding</a>              </p>

              </p>
              <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex analysis </a>
              <a class="keyword"> non-convex optimization </a>
                </p>
              </td>
            </tr>
        
        
       
      
      
      	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/deep_dual.jpg" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="http://proceedings.mlr.press/v139/ergen21b.html">
        <papertitle><strong>Revealing the Structure of Deep Neural Networks via Convex Duality</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>ICML 2021 </em>&nbsp; <br>    </p>
		<a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2002.09773">arXiv</a>              
		<a class="link link_pdf" rel="external" href="http://proceedings.mlr.press/v139/ergen21b">Proceeding</a>              </p>
              </p>
     <p>
              <a class="keyword"> deep neural networks </a>
              <a class="keyword"> convex duality </a>
              <a class="keyword"> non-convex optimization </a>
                </p>
              </td>
            </tr>
        
            	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/parallel_network.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="http://proceedings.mlr.press/v139/ergen21a.html">
        <papertitle><strong>Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>ICML 2021 </em>&nbsp; <br>     </p>
		<a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2110.05518">arXiv</a>              
		<a class="link link_pdf" rel="external"  href="http://proceedings.mlr.press/v139/ergen21a">Proceeding</a>              </p>
     <p>
              <a class="keyword"> deep neural networks </a>
              <a class="keyword"> convex optimization </a>
     </p>
              </td>
            </tr>
        
          
      
      
    

         	<tr>
          <td width="30%" >
            <div class="one">
                <img src="images/convex_cnn.jpg" style="width:200px;height:150px;">
            </div>                
              </td>
  
              <td valign="middle" width="70%">
		  <p><a href="https://openreview.net/pdf?id=0N8jUH4JMv6">
        <papertitle><strong>Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>ICLR 2021</em> <font color="red"><b>(Spotlight Presentation) </b></font>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/2006.14798">arXiv</a>              
		<a class="link link_pdf" rel="external" href="https://openreview.net/pdf?id=0N8jUH4JMv6">Proceeding</a>              </p>
              </p>
                  <p></p>
              <p>
              <a class="keyword"> convolutional neural networks </a>
              <a class="keyword"> convex optimization </a>
              <a class="keyword"> deep learning </a>
              </p>
              </td>
            </tr>
          
          
          
          
            	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_nn_vector.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://openreview.net/pdf?id=fGF8qAqpXXG">
        <papertitle> <strong>Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms</strong></papertitle></a><br>
Arda Sahiner, <strong>Tolga Ergen</strong>, John Pauly, Mert Pilanci<br/>
                <em>ICLR 2021 </em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/2012.13329">arXiv</a>              
		<a class="link link_pdf" rel="external" href="https://openreview.net/pdf?id=fGF8qAqpXXG">Proceeding</a>              </p>
              </p>
          <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex analysis </a>
              <a class="keyword"> non-convex optimization </a>
          </p>
              </td>
            </tr>
          
          
          
                   	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_cnn_twolayer.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://opt-ml.org/papers/2020/paper_39.pdf">
        <papertitle><strong>Convex Programs for Global Optimization of Convolutional Neural
Networks in Polynomial-Time</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>NeurIPS 2020 Workshop on Optimization for Machine Learning </em><font color="red"><b>(Oral Presentation) </b></font>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://opt-ml.org/papers/2020/paper_39.pdf">PDF</a>              </p>
              </p>
          <p>
              <a class="keyword"> convolutional neural networks </a>
              <a class="keyword"> convex optimization </a>
              <a class="keyword"> deep learning </a>
          </p>
              </td>
            </tr>
          
          
          
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_nn.jpg" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="http://proceedings.mlr.press/v119/pilanci20a.html">
        <papertitle><strong>Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks</strong></papertitle></a><br>
Mert Pilanci, <strong>Tolga Ergen</strong><br/>
                <em>ICML 2020</em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external"  href="https://arxiv.org/abs/2002.10553">arXiv</a>  
		<a class="link link_pdf" rel="external"  href="http://proceedings.mlr.press/v119/pilanci20a.html">Proceeding</a>              </p>
              </p>
                <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex analysis </a>
              <a class="keyword"> non-convex optimization </a>
                </p>
              </td>
            </tr>
        
 

       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_geometry.jpeg" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
                		  <p><a href="http://proceedings.mlr.press/v108/ergen20a.html">
        <papertitle></papertitle><strong>Convex Geometry of Two-Layer ReLU Networks: Implicit Autoencoding and Interpretable Models</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>AISTATS 2020</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="http://proceedings.mlr.press/v108/ergen20a.html">Proceeding</a>              </p>
              </p>
                <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex analysis </a>
              <a class="keyword"> non-convex optimization </a>
                </p>
              </td>
            </tr>
        
        
        
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_autoreg.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/9413662">
        <papertitle><strong>Convex Neural Autoregressive Models: Towards Tractable, Expressive, and Theoretically-Backed Models for Sequential Forecasting and Generator</strong></papertitle></a><br>
Vikul Gupta, Burak Bartan, <strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>ICASSP 2021</em>&nbsp; <font color="red"><b>(Outstanding Paper Award) </b></font>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/document/9413662">Proceeding</a>              </p>
              </p>
                <p>
              <a class="keyword"> generative models </a>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex optimization </a>
                </p>
              </td>
            </tr>



       

       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/distributed_svm.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1051200420300026">
        <papertitle><strong>A Novel Distributed Anomaly Detection Algorithm Based on Support Vector Machines</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>Digital Signal Processing</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="https://www.sciencedirect.com/science/article/abs/pii/S1051200420300026">Proceeding</a>              </p>
              </p>
            <p>
              <a class="keyword"> support vector machines </a>
              <a class="keyword"> distributed optimization </a>
            </p>
              </td>
            </tr>
        
        
        
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_cutting.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://opt-ml.org/papers/2019/paper_34.pdf">
        <papertitle><strong>Convex Duality and Cutting Plane Methods for Over-parameterized Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>NeurIPS 2019 Workshop on Optimization for Machine Learning  </em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://opt-ml.org/papers/2019/paper_34.pdf">PDF</a>              </p>
              </p>
          <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex analysis </a>
              <a class="keyword"> non-convex optimization </a>                </p>
              </td>
            </tr>

     
         	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/random_proj.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://drive.google.com/file/d/1x_ft76LXMD9-OKGnkdcJWcFLyXaPxo2x/view">
        <papertitle><strong>Random Projections for Learning Non-convex Models</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mert Pilanci<br/>
                <em>NeurIPS 2019 Workshop on Beyond First Order Methods in Machine Learning </em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="https://drive.google.com/file/d/1x_ft76LXMD9-OKGnkdcJWcFLyXaPxo2x/view">PDF</a>              </p>
              </p>
          <p>
              <a class="keyword"> randomized algorithms </a>
              <a class="keyword"> non-convex optimization </a>
          </p>
              </td>
            </tr>
          
          
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/convex_shallow.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/abstract/document/8919769">
        <papertitle><strong>Convex Optimization for Shallow Neural Networks</strong></papertitle></a><br>
 <strong>Tolga Ergen</strong>, Mert Pilanci <br/>
                <em>ALLERTON 2019</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="https://ieeexplore.ieee.org/abstract/document/8919769">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> convex optimization </a>
              </p>
              </td>
            </tr>
        
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/energy_efficient.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/abstract/document/8836527">
        <papertitle><strong>Energy-Efficient LSTM Networks for Online Learning</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Ali Mirza, Serdar Kozat<br/>
                <em>IEEE TNNNLS</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/abstract/document/8836527">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> recurrent neural networks </a>
              <a class="keyword"> online learning </a>
              <a class="keyword"> non-convex optimization </a>
              </p>
              </td>
            </tr>

       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/unsup_anomaly.png" style="width:200px;height:100px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
                		  <p><a href="https://ieeexplore.ieee.org/abstract/document/8836638">
        <papertitle><strong>Unsupervised Anomaly Detection with LSTM Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>IEEE TNNLS</em>&nbsp; <br> </p>
		<a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/1710.09207">arXiv</a>              
		<a class="link link_pdf" rel="external" href="https://ieeexplore.ieee.org/abstract/document/8836638">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> recurrent neural networks </a>
              <a class="keyword"> support vector machines </a>
              <a class="keyword"> non-convex optimization </a>
              </p>
              </td>
            </tr>
        
  
  
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/distributed_tree.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
                		  <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0165168418302688">
        <papertitle><strong>Team-Optimal Online Estimation of Dynamic Parameters over Distributed Tree Networks</strong></papertitle></a><br>
Fatih Kilic, <strong>Tolga Ergen</strong>, Muhammed Sayin, Serdar Kozat.<br/>
                <em>Signal Processing</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://www.sciencedirect.com/science/article/abs/pii/S0165168418302688">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> online learning </a>
              <a class="keyword"> distributed optimization </a>
              </p>
              </td>
            </tr>
        
       
        
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/distributed_lstm.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/8169114">
        <papertitle><strong>Online Training of LSTM Networks in Distributed Systems for Variable Length Data Sequences</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>IEEE TNNLS</em>&nbsp; <br> </p>
                <a class="link link_arxiv" rel="external" href="https://arxiv.org/abs/1710.08744">arXiv</a>             
                <a class="link link_pdf" rel="external" href="https://ieeexplore.ieee.org/document/8169114">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> recurrent neural networks </a>
              <a class="keyword"> distributed optimization </a>
              </p>
              </td>
            </tr>



       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/online_lstm.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/abstract/document/8036280">
        <papertitle><strong>Efficient Online Learning Algorithms Based on LSTM Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>IEEE TNNLS</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/abstract/document/8036280">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> recurrent neural networks </a>
              <a class="keyword"> online learning </a>
              </p>
              </td>
            </tr>

       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/siu_efficient_regression.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/8404708">
        <papertitle><strong>A Highly Efficient Recurrent Neural Network Architecture for Data Regression</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Emir Ceyani<br/>
                <em>IEEE SIU 2018</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/document/8404708">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> recurrent neural networks </a>
              <a class="keyword"> online learning </a>
              </p>
              </td>
            </tr>
        
        
        
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/siu_novel_anomaly.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/8404676/">
        <papertitle> <strong>A Novel Anomaly Detection Approach Based on Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Mine Kerpicci<br/>
                <em>IEEE SIU 2018</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/document/8404676/">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword">  neural networks </a>
              <a class="keyword"> non-convex optimization </a>
              </p>
              </td>
            </tr>


        <tr>
          <td width="25%">
            <div class="one">
                <img src="images/siu_2.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
      <p><a href="https://ieeexplore.ieee.org/document/8404806">
        <papertitle> <strong>Recurrent neural networks based online learning algorithms for distributed systems
</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Safa Sahin, Serdar Kozat<br/>
                <em>IEEE SIU 2018</em>&nbsp; <br> </p>
    <a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/document/8404806">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword">  recurrent neural networks </a>
              <a class="keyword"> distributed systems </a>
              </p>
              </td>
            </tr>

      
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/eusipco_lstm.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="http://150.162.46.34:8080/eusipco2017/papers/1570346404.pdf">
        <papertitle> <strong>Computationally Efficient Online Regression via LSTM Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>EUSIPCO 2017</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="http://150.162.46.34:8080/eusipco2017/papers/1570346404.pdf">PDF</a>              </p>
              </p>
              <p>
              <a class="keyword"> recurrent neural networks </a>
              <a class="keyword"> online learning </a>
              </p>
              </td>
            </tr>


      
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/bandit.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/7960214/">
        <papertitle><strong>An Efficient Bandit Algorithm for General Weight Assignments </strong></papertitle></a><br>
Kaan Gokcesu, <strong>Tolga Ergen</strong>, Selami Ciftci, Serdar Kozat<br/>
                <em>IEEE SIU 2017</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="https://ieeexplore.ieee.org/document/7960214/">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> adversarial multi armed bandit </a>
              <a class="keyword"> non-convex optimization </a>
              </p>
              </td>
            </tr>


      
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/siu_online.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/7960218">
        <papertitle><strong> Neural Networks Based Online Learning</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>IEEE SIU 2017</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="https://ieeexplore.ieee.org/document/7960218">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> online learning </a>
              </p>
              </td>
            </tr>

      
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/soft_novelty.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/7960213">
        <papertitle><strong>Novelty Detection Using Soft Partitioning and Hierarchical Models</strong></papertitle></a><br>
 <strong>Tolga Ergen</strong>, Kaan Gokcesu, Mustafa Simsek, Serdar Kozat<br/>
 <em>IEEE SIU 2017</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external" href="https://ieeexplore.ieee.org/document/7960213">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> online learning </a>
              <a class="keyword"> non-convex optimization </a>
              </p>
              </td>
            </tr>


      
       	<tr>
          <td width="25%">
            <div class="one">
                <img src="images/siu_distributed.png" style="width:200px;height:150px;">
            </div>                
              </td>
              <td valign="middle" width="75%">
		  <p><a href="https://ieeexplore.ieee.org/document/7960220">
        <papertitle><strong>Online Distributed Nonlinear Regression via Neural Networks</strong></papertitle></a><br>
<strong>Tolga Ergen</strong>, Serdar Kozat<br/>
                <em>IEEE SIU 2017</em>&nbsp; <br> </p>
		<a class="link link_pdf" rel="external"  href="https://ieeexplore.ieee.org/document/7960220">Proceeding</a>              </p>
              </p>
              <p>
              <a class="keyword"> neural networks </a>
              <a class="keyword"> distributed optimization </a>
              </p>
              </td>
            </tr>

        </tbody></table>

           <br>
        

        
    
    </div>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>




